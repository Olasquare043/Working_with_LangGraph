{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39084a73",
   "metadata": {},
   "source": [
    "## Working with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dfa7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful\n"
     ]
    }
   ],
   "source": [
    "# loading neccessary libraries\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image,display\n",
    "import os \n",
    "\n",
    "print(\"âœ… All imports successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13721fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# initialize the LLM\n",
    "llm= ChatOpenAI(model=\"gpt-4o-mini\",temperature=0.7)\n",
    "print(f\"âœ… LLM initialized: {llm.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347e7d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Assistant node defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt that defines assistance behavior\n",
    "sys_msg=SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\")\n",
    "def assistance(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistance node- process message and generate response\n",
    "    \"\"\"\n",
    "    # Combine the system prompt with conversation history\n",
    "    messages= [sys_msg]+ state[\"messages\"]\n",
    "\n",
    "    # Get response from llm\n",
    "    response= llm.invoke(messages)\n",
    "\n",
    "    # return as state update\n",
    "    return {\"messages\":[AIMessage(content=response.content)]}\n",
    "print(\"âœ… Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604f46c",
   "metadata": {},
   "source": [
    "### Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274d9d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Graph structure defined\n"
     ]
    }
   ],
   "source": [
    "# Create a StateGraph with MessageState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistance node\n",
    "builder.add_node(\"assistance\", assistance)\n",
    "\n",
    "# Now let define the flow:\n",
    "# START â†’ assistance â†’ END\n",
    "builder.add_edge(START, \"assistance\")\n",
    "builder.add_edge(\"assistance\", END)\n",
    "\n",
    "print(\"âœ… Graph structure defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba91cd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Agent compiled with memory\n"
     ]
    }
   ],
   "source": [
    "# Adding memory\n",
    "memory=MemorySaver()\n",
    "\n",
    "# Compile the graph with memeory\n",
    "agent= builder.compile(checkpointer=memory)\n",
    "print(\"âœ… Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59153a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not display graph: Failed to reach https://mermaid.ink API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "Graph structure: START â†’ assistant â†’ END\n"
     ]
    }
   ],
   "source": [
    "# Let visualize our graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START â†’ assistant â†’ END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26243d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with session ID: user001\n"
     ]
    }
   ],
   "source": [
    "# let create a session id and test run our agent\n",
    "session_id= \"user001\"\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6bf4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Conversation function ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function for conversatios\n",
    "def run_chat(user_input:str, thread_id:str = session_id):\n",
    "    \"\"\"Send a message to the agent and get response.\"\"\"\n",
    "    # Invoke the agent\n",
    "    result= agent.invoke({\"messages\":[HumanMessage(content=user_input)]},\n",
    "    config={\"configurable\":{\"thread_id\":thread_id}})\n",
    "\n",
    "    # print the conversation \n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "             print(f\"\\nðŸ‘¤ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ðŸ¤– Agent: {message.content}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Conversation function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98478ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "run_chat(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b72d0",
   "metadata": {},
   "source": [
    "#### Testing if it remember conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message\n",
    "run_chat(\"My name is Saheed Olayinka Olayemi, popularly know as Olasquare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: Hello, who are you?\n",
      "ðŸ¤– Agent: Hello! I'm your friendly assistant, here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed Olayinka Olayemi, popularly know as Olasquare\n",
      "ðŸ¤– Agent: Nice to meet you, Olasquare! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is ?\n",
      "ðŸ¤– Agent: Your name is Saheed Olayinka Olayemi, but you are popularly known as Olasquare. How can I help you further?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Second message\n",
    "run_chat(\"My name is ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: Hello, who are you?\n",
      "ðŸ¤– Agent: Hello! I'm your friendly assistant, here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed Olayinka Olayemi, popularly know as Olasquare\n",
      "ðŸ¤– Agent: Nice to meet you, Olasquare! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is ?\n",
      "ðŸ¤– Agent: Your name is Saheed Olayinka Olayemi, but you are popularly known as Olasquare. How can I help you further?\n",
      "\n",
      "ðŸ‘¤ User: I'm learning about RAG systems\n",
      "ðŸ¤– Agent: That's great! RAG systems, or Retrieval-Augmented Generation systems, combine retrieval techniques with generative models to enhance the quality of responses. They retrieve relevant information from a database or knowledge base and then use that information to generate more accurate and contextually relevant responses. \n",
      "\n",
      "If you have specific questions about RAG systems or if there's a particular aspect you'd like to learn more about, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Start a new topic\n",
    "run_chat(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05145013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: Hello, who are you?\n",
      "ðŸ¤– Agent: Hello! I'm your friendly assistant, here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed Olayinka Olayemi, popularly know as Olasquare\n",
      "ðŸ¤– Agent: Nice to meet you, Olasquare! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is ?\n",
      "ðŸ¤– Agent: Your name is Saheed Olayinka Olayemi, but you are popularly known as Olasquare. How can I help you further?\n",
      "\n",
      "ðŸ‘¤ User: I'm learning about RAG systems\n",
      "ðŸ¤– Agent: That's great! RAG systems, or Retrieval-Augmented Generation systems, combine retrieval techniques with generative models to enhance the quality of responses. They retrieve relevant information from a database or knowledge base and then use that information to generate more accurate and contextually relevant responses. \n",
      "\n",
      "If you have specific questions about RAG systems or if there's a particular aspect you'd like to learn more about, feel free to ask!\n",
      "\n",
      "ðŸ‘¤ User: Can you explain the main components?\n",
      "ðŸ¤– Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a large knowledge base or database based on a user's query. It often uses techniques like keyword matching, vector similarity, or embedding-based retrieval.\n",
      "\n",
      "2. **Generator Component**: \n",
      "   - After retrieval, this component uses a generative model (like a transformer-based model) to create a coherent and contextually appropriate response. The generator takes both the user's query and the retrieved documents as input.\n",
      "\n",
      "3. **Knowledge Base**: \n",
      "   - This is the collection of documents or data from which information is retrieved. It can be structured (like databases) or unstructured (like text corpora).\n",
      "\n",
      "4. **Integration Mechanism**: \n",
      "   - This is the process that combines the retrieved information with the generative model's capabilities. It ensures that the response is contextually accurate and relevant by effectively merging the retrieved content with the generated text.\n",
      "\n",
      "5. **Feedback Loop (optional)**: \n",
      "   - Some RAG systems incorporate a feedback mechanism that allows the model to learn from user interactions over time, improving retrieval accuracy and response quality.\n",
      "\n",
      "These components work together to create a system that can provide informative, context-aware responses by leveraging both retrieval and generation techniques. If you have more questions or need further details on any specific component, let me know!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reference it\n",
    "run_chat(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68743fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ User: Hello, who are you?\n",
      "ðŸ¤– Agent: Hello! I'm your friendly assistant, here to help you with any questions or information you need. How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed Olayinka Olayemi, popularly know as Olasquare\n",
      "ðŸ¤– Agent: Nice to meet you, Olasquare! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: My name is ?\n",
      "ðŸ¤– Agent: Your name is Saheed Olayinka Olayemi, but you are popularly known as Olasquare. How can I help you further?\n",
      "\n",
      "ðŸ‘¤ User: I'm learning about RAG systems\n",
      "ðŸ¤– Agent: That's great! RAG systems, or Retrieval-Augmented Generation systems, combine retrieval techniques with generative models to enhance the quality of responses. They retrieve relevant information from a database or knowledge base and then use that information to generate more accurate and contextually relevant responses. \n",
      "\n",
      "If you have specific questions about RAG systems or if there's a particular aspect you'd like to learn more about, feel free to ask!\n",
      "\n",
      "ðŸ‘¤ User: Can you explain the main components?\n",
      "ðŸ¤– Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a large knowledge base or database based on a user's query. It often uses techniques like keyword matching, vector similarity, or embedding-based retrieval.\n",
      "\n",
      "2. **Generator Component**: \n",
      "   - After retrieval, this component uses a generative model (like a transformer-based model) to create a coherent and contextually appropriate response. The generator takes both the user's query and the retrieved documents as input.\n",
      "\n",
      "3. **Knowledge Base**: \n",
      "   - This is the collection of documents or data from which information is retrieved. It can be structured (like databases) or unstructured (like text corpora).\n",
      "\n",
      "4. **Integration Mechanism**: \n",
      "   - This is the process that combines the retrieved information with the generative model's capabilities. It ensures that the response is contextually accurate and relevant by effectively merging the retrieved content with the generated text.\n",
      "\n",
      "5. **Feedback Loop (optional)**: \n",
      "   - Some RAG systems incorporate a feedback mechanism that allows the model to learn from user interactions over time, improving retrieval accuracy and response quality.\n",
      "\n",
      "These components work together to create a system that can provide informative, context-aware responses by leveraging both retrieval and generation techniques. If you have more questions or need further details on any specific component, let me know!\n",
      "\n",
      "ðŸ‘¤ User: Which component is most important?\n",
      "ðŸ¤– Agent: The importance of each component in a Retrieval-Augmented Generation (RAG) system can vary depending on the specific use case and requirements. However, generally speaking:\n",
      "\n",
      "1. **Retrieval Component**: Often considered the most critical part because the quality and relevance of the retrieved information directly impact the overall response quality. If the retrieval isn't effective, the generative model may produce responses based on inaccurate or irrelevant information.\n",
      "\n",
      "2. **Generator Component**: Equally important, as it is responsible for crafting coherent and contextually appropriate responses. A strong generative model can articulate the retrieved information effectively.\n",
      "\n",
      "In summary, both components are vital to the system's success. The retrieval component ensures that accurate context is provided, while the generator transforms that context into a human-readable response. The balance and integration between these two components ultimately determine the system's overall performance.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "run_chat(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c00767",
   "metadata": {},
   "source": [
    "### Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fcfb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ CONVERSATION 1\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed\n",
      "ðŸ¤– Agent: Nice to meet you, Saheed! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ðŸŸ¢ CONVERSATION 2\n",
      "\n",
      "ðŸ‘¤ User: My name is Olayemi\n",
      "ðŸ¤– Agent: Nice to meet you, Olayemi! How can I assist you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nðŸ”µ CONVERSATION 1\")\n",
    "run_chat(\"My name is Saheed\", thread_id=\"user_saheed\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nðŸŸ¢ CONVERSATION 2\")\n",
    "run_chat(\"My name is Olayemi\", thread_id=\"user_ola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76d828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ BACK TO CONVERSATION 1\n",
      "\n",
      "ðŸ‘¤ User: My name is Saheed\n",
      "ðŸ¤– Agent: Nice to meet you, Saheed! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: What's my name?\n",
      "ðŸ¤– Agent: Your name is Saheed. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Saheed - does it remember his name?\n",
    "print(\"\\nðŸ”µ BACK TO CONVERSATION 1\")\n",
    "run_chat(\"What's my name?\", thread_id=\"user_saheed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ae3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ BACK TO CONVERSATION 2\n",
      "\n",
      "ðŸ‘¤ User: My name is Olayemi\n",
      "ðŸ¤– Agent: Nice to meet you, Olayemi! How can I assist you today?\n",
      "\n",
      "ðŸ‘¤ User: What's my name?\n",
      "ðŸ¤– Agent: Your name is Olayemi. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Olayemi - does it remember his name?\n",
    "print(\"\\nðŸ”µ BACK TO CONVERSATION 2\")\n",
    "run_chat(\"What's my name?\", thread_id=\"user_ola\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
